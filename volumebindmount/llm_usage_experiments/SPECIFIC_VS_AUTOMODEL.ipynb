{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3958133",
   "metadata": {},
   "source": [
    "### Specific Model Loading vs. AutoModel Loading\n",
    "\n",
    "* This is an attempt to compare specific model loading using {ModelName}ForCausalLM.from_pretrained() to AutoModelForCausalLM.from_pretrained() to determine if there are any problems, specifically with the Falcon model.\n",
    "* We will use Falcon1B because this is being done on a computer with a small GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d76526e",
   "metadata": {},
   "source": [
    "#### Check CUDA Availability\n",
    "\n",
    "* We first need to check to ensure that CUDA is available.  We can start with the nvidia-smi shell tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b36e2f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jan 16 21:00:41 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.146.02             Driver Version: 535.146.02   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1070        Off | 00000000:01:00.0 Off |                  N/A |\n",
      "| 27%   32C    P8               6W / 151W |     16MiB /  8192MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3e0948",
   "metadata": {},
   "source": [
    "#### Check Torch Capability\n",
    "\n",
    "* We're going to need to be able to install Torch.\n",
    "* If you check out the README-BOOT file at https://github.com/pwdel/gpu-jupyter-tensorflow/tree/main it should help you get this going.\n",
    "* Long story short, you need to install torch with the -U flag to get the right version, and you may need to restart the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9efb1819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages (2.1.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: sympy in /opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages (from torch) (2023.9.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages (from torch) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages (from torch) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.3.101)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba93e2ff",
   "metadata": {},
   "source": [
    "* The version we're looking for is `2.1.2+cu121`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a99f529",
   "metadata": {},
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c094e15",
   "metadata": {},
   "source": [
    "* Moreover, the following command sshould show the GPU and the memory capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7829dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.2+cu121\n",
      "GPU is available.\n",
      "Current GPU device: 0\n",
      "GPU Name: NVIDIA GeForce GTX 1070\n",
      "GPU Memory Capacity: 8.501919744 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available.\")\n",
    "else:\n",
    "    print(\"GPU is not available. Check your CUDA installation.\")\n",
    "\n",
    "current_device = torch.cuda.current_device()\n",
    "print(f\"Current GPU device: {current_device}\")\n",
    "\n",
    "device_properties = torch.cuda.get_device_properties(0)  # Replace 0 with the desired GPU index\n",
    "print(f\"GPU Name: {device_properties.name}\")\n",
    "print(f\"GPU Memory Capacity: {device_properties.total_memory / 1e9} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29307ab0",
   "metadata": {},
   "source": [
    "#### Automatic Model Loading\n",
    "\n",
    "* First off, we will try to do this with auto model loading, which we have been able to do successfully previously.\n",
    "* We start out by importing ouf transformer libraries for AutoModelForCausalLM, AutoTokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "208cce4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# imports - Auto{Stuff}\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942f9308",
   "metadata": {},
   "source": [
    "* Having imported the right tools, we can ensure that we're loading from https://huggingface.co/tiiuae/falcon-rw-1b, which is a 1B parameter and should not take up much memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd3df8db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading tokenizer_config.json: 100%|██████████| 234/234 [00:00<00:00, 44.2kB/s]\n",
      "Downloading vocab.json: 100%|██████████| 798k/798k [00:00<00:00, 5.49MB/s]\n",
      "Downloading merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 13.3MB/s]\n",
      "Downloading tokenizer.json: 100%|██████████| 2.11M/2.11M [00:00<00:00, 8.07MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 99.0/99.0 [00:00<00:00, 47.6kB/s]\n",
      "Downloading config.json: 100%|██████████| 1.05k/1.05k [00:00<00:00, 846kB/s]\n",
      "Downloading (…)figuration_falcon.py: 100%|██████████| 6.70k/6.70k [00:00<00:00, 3.10MB/s]\n",
      "A new version of the following files was downloaded from https://huggingface.co/tiiuae/falcon-rw-1b:\n",
      "- configuration_falcon.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "Downloading modeling_falcon.py: 100%|██████████| 56.9k/56.9k [00:00<00:00, 2.00MB/s]\n",
      "A new version of the following files was downloaded from https://huggingface.co/tiiuae/falcon-rw-1b:\n",
      "- modeling_falcon.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "Downloading pytorch_model.bin: 100%|██████████| 2.62G/2.62G [03:47<00:00, 11.6MB/s]\n",
      "Downloading generation_config.json: 100%|██████████| 115/115 [00:00<00:00, 18.1kB/s]\n"
     ]
    }
   ],
   "source": [
    "model_identifier = \"tiiuae/falcon-rw-1b\"\n",
    "# tokenizers are generally lightweight and are loaded into RAM. They are used to convert text into a format that the model can understand (like token IDs).\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_identifier)\n",
    "# if a GPU is available and PyTorch is configured to use it, the model will be loaded into the GPU's memory. \n",
    "model = AutoModelForCausalLM.from_pretrained(model_identifier, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c1111f",
   "metadata": {},
   "source": [
    "* from_pretrained method in the Hugging Face Transformers library does not automatically move the model to a GPU device.\n",
    "* Hence, given that torch was shown to be available above, we should move the model to cuda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "620b7e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move model to GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to('cuda:0')  # Move model to the first GPU device\n",
    "else:\n",
    "    print(\"No GPU available, using CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af52702",
   "metadata": {},
   "source": [
    "* We may then verify that the model has been moved to the device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0bd2000f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = next(model.parameters()).device\n",
    "print(f\"Model is on device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fc759c",
   "metadata": {},
   "source": [
    "* The above demonstrates that we can use AutoModel loading to move falcon to the GPU. We may now delete the model from the device.\n",
    "* If we want to go even more specific, we can look at how much memory it is taking up in the GPU specifically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "837f817a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pynvml\n",
      "  Downloading pynvml-11.5.0-py3-none-any.whl (53 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m524.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pynvml\n",
      "Successfully installed pynvml-11.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pynvml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fcf0a4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pynvml\n",
    "import logging\n",
    "\n",
    "# Initialize pynvml library\n",
    "pynvml.nvmlInit()\n",
    "\n",
    "def log_gpu_stats():\n",
    "    try:\n",
    "        gpu_count = pynvml.nvmlDeviceGetCount()\n",
    "        print(f\"Number of GPUs: {gpu_count}\")\n",
    "\n",
    "        for i in range(gpu_count):\n",
    "            print(i)\n",
    "            handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n",
    "            name = pynvml.nvmlDeviceGetName(handle)\n",
    "            temperature = pynvml.nvmlDeviceGetTemperature(handle, pynvml.NVML_TEMPERATURE_GPU)\n",
    "            memory_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "            utilization = pynvml.nvmlDeviceGetUtilizationRates(handle)\n",
    "\n",
    "            print(f\"GPU {i + 1} - Name: {name}, Temperature: {temperature}°C,\"\n",
    "                        f\" Memory Used: {memory_info.used / 1024 / 1024} MB,\"\n",
    "                        f\" GPU Utilization: {utilization.gpu}%, Memory Utilization: {utilization.memory}%\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error logging GPU stats: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a628191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs: 1\n",
      "0\n",
      "GPU 1 - Name: NVIDIA GeForce GTX 1070, Temperature: 32°C, Memory Used: 5190.4375 MB, GPU Utilization: 0%, Memory Utilization: 0%\n"
     ]
    }
   ],
   "source": [
    "log_gpu_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9bddbec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ed36b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs: 1\n",
      "0\n",
      "GPU 1 - Name: NVIDIA GeForce GTX 1070, Temperature: 32°C, Memory Used: 5190.4375 MB, GPU Utilization: 0%, Memory Utilization: 0%\n"
     ]
    }
   ],
   "source": [
    "log_gpu_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28783534",
   "metadata": {},
   "source": [
    "#### Specific Model Loading\n",
    "\n",
    "* Next we will try to do this with auto model loading, which we have been able to do successfully previously.\n",
    "* As we can see, the specific model loading library appears to be no longer available in transformers 4.29.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b75e4f19",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'FalconForCausalLM' from 'transformers' (/opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# imports - Auto{Stuff}\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, FalconForCausalLM\n\u001b[1;32m      5\u001b[0m model_identifier \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtiiuae/falcon-rw-1b\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m FalconForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_identifier, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'FalconForCausalLM' from 'transformers' (/opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "# imports - Auto{Stuff}\n",
    "import torch\n",
    "from transformers import AutoTokenizer, FalconForCausalLM\n",
    "\n",
    "model_identifier = \"tiiuae/falcon-rw-1b\"\n",
    "model = FalconForCausalLM.from_pretrained(model_identifier, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "13975fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.29.2\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6af5418",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu_env",
   "language": "python",
   "name": "tf_gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

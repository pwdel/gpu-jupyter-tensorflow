{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1694e2fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-04 18:30:12,773 - INFO - nvidia-smi is working and GPUs are detected.\n",
      "2023-12-04 18:30:12,774 - INFO - GPU: 0  NVIDIA GeForce, Count: 1\n",
      "2023-12-04 18:30:12,775 - INFO - Driver Version: 535.129.03\n",
      "2023-12-04 18:30:12,775 - INFO - cuDNN is available.\n",
      "2023-12-04 18:30:12,776 - INFO - cuDNN Version: 8902\n",
      "2023-12-04 18:30:12,776 - INFO - CUDA Availability: True\n",
      "2023-12-04 18:30:12,777 - INFO - Device set to: cuda\n",
      "2023-12-04 18:30:12,778 - INFO - Logging -- CUDA Toolkit Version (used by PyTorch): 12.1\n",
      "2023-12-04 18:30:12,778 - INFO - Passed nvidia_smi_working, cuda_available, cudnn_available\n",
      "2023-12-04 18:30:12,779 - INFO - Number of GPUs: 1\n",
      "2023-12-04 18:30:12,780 - INFO - GPU 1 - Name: NVIDIA GeForce GTX 1070, Temperature: 33°C, Memory Used: 3436.4375 MB, GPU Utilization: 0%, Memory Utilization: 0%\n",
      "2023-12-04 18:30:12,781 - INFO - All hardware checks passed. Ready for inference.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import re\n",
    "import torch\n",
    "import pynvml\n",
    "import logging\n",
    "from collections import Counter\n",
    "\n",
    "# Initialize pynvml library\n",
    "pynvml.nvmlInit()\n",
    "\n",
    "class HardwareCheck:\n",
    "\n",
    "    def __init__(self):\n",
    "        # self.log_nvidia_smi()  # This method doesn't perform tests, so it's fine to keep\n",
    "        # self.log_cuda_toolkit_version()\n",
    "        pass\n",
    "\n",
    "    def test_nvidia_smi(self):\n",
    "        try:\n",
    "            nvidia_smi_output = subprocess.check_output(['nvidia-smi']).decode()\n",
    "\n",
    "            # Extracting GPU names\n",
    "            gpu_names = re.findall(r'(\\d+\\s+NVIDIA [^\\s]+)', nvidia_smi_output)\n",
    "            gpu_count = Counter(gpu_names)\n",
    "\n",
    "            # Parsing the driver version\n",
    "            driver_version_match = re.search(r'Driver Version: (\\d+\\.\\d+\\.\\d+)', nvidia_smi_output)\n",
    "            driver_version = driver_version_match.group(1) if driver_version_match else \"Unknown\"\n",
    "\n",
    "            # Checking if at least one GPU is detected\n",
    "            self.nvidia_smi_working = len(gpu_count) > 0\n",
    "            if self.nvidia_smi_working:\n",
    "                logging.info(\"nvidia-smi is working and GPUs are detected.\")\n",
    "                for gpu, count in gpu_count.items():\n",
    "                    logging.info(f\"GPU: {gpu}, Count: {count}\")\n",
    "                logging.info(f\"Driver Version: {driver_version}\")\n",
    "            else:\n",
    "                logging.warning(\"No GPUs detected by nvidia-smi.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            self.nvidia_smi_working = False\n",
    "            logging.error(f\"Error running nvidia-smi: {e}\")\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "    def test_cudnn_availability(self):\n",
    "        # Check for cuDNN availability and log it\n",
    "        self.cudnn_available = torch.backends.cudnn.is_available()\n",
    "        if self.cudnn_available:\n",
    "            self.cudnn_version = torch.backends.cudnn.version()\n",
    "            logging.info(\"cuDNN is available.\")\n",
    "            logging.info(f\"cuDNN Version: {self.cudnn_version}\")\n",
    "        else:\n",
    "            logging.info(\"cuDNN is not available.\")\n",
    "        return self\n",
    "\n",
    "\n",
    "    def test_cuda_availability(self):\n",
    "        # Check for CUDA availability and log it\n",
    "        if torch.cuda.is_available():\n",
    "            self.cuda_available = True\n",
    "            self.device = torch.device(\"cuda\")\n",
    "        else:\n",
    "            self.cuda_available = False\n",
    "            self.device = torch.device(\"cpu\")\n",
    "            raise RuntimeError(\"CUDA is not available and fallback to CPU is not desired.\")\n",
    "\n",
    "        logging.info(f\"CUDA Availability: {self.cuda_available}\")\n",
    "        logging.info(f\"Device set to: {self.device}\")\n",
    "        return self\n",
    "\n",
    "\n",
    "    def log_gpu_stats(self):\n",
    "        try:\n",
    "            gpu_count = pynvml.nvmlDeviceGetCount()\n",
    "            logging.info(f\"Number of GPUs: {gpu_count}\")\n",
    "\n",
    "            for i in range(gpu_count):\n",
    "                handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n",
    "                name = pynvml.nvmlDeviceGetName(handle)  # Remove .decode('utf-8')\n",
    "                temperature = pynvml.nvmlDeviceGetTemperature(handle, pynvml.NVML_TEMPERATURE_GPU)\n",
    "                memory_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "                utilization = pynvml.nvmlDeviceGetUtilizationRates(handle)\n",
    "\n",
    "                logging.info(f\"GPU {i + 1} - Name: {name}, Temperature: {temperature}°C,\"\n",
    "                            f\" Memory Used: {memory_info.used / 1024 / 1024} MB,\"\n",
    "                            f\" GPU Utilization: {utilization.gpu}%, Memory Utilization: {utilization.memory}%\")\n",
    "        except Exception as e:\n",
    "            logging.info(f\"Error logging GPU stats: {e}\")\n",
    "\n",
    "\n",
    "    def log_nvidia_smi(self):\n",
    "        try:\n",
    "            nvidia_smi_output = subprocess.check_output(['nvidia-smi']).decode()\n",
    "\n",
    "            # Extracting GPU names\n",
    "            gpu_names = re.findall(r'(\\d+\\s+NVIDIA [^\\s]+)', nvidia_smi_output)\n",
    "            gpu_count = Counter(gpu_names)\n",
    "\n",
    "            # Parsing the driver version\n",
    "            driver_version_match = re.search(r'Driver Version: (\\d+\\.\\d+\\.\\d+)', nvidia_smi_output)\n",
    "            driver_version = driver_version_match.group(1) if driver_version_match else \"Unknown\"\n",
    "\n",
    "            # Logging GPU counts and names\n",
    "            for gpu, count in gpu_count.items():\n",
    "                logging.info(f\"GPU: {gpu}, Count: {count}\")\n",
    "\n",
    "            logging.info(f\"Driver Version: {driver_version}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.info(f\"Error running nvidia-smi: {e}\")\n",
    "\n",
    "            \n",
    "    def log_cuda_toolkit_version(self):\n",
    "        # Check for CUDA Toolkit version used by PyTorch\n",
    "        cuda_version = torch.version.cuda\n",
    "        logging.info(f\"Logging -- CUDA Toolkit Version (used by PyTorch): {cuda_version}\")\n",
    "\n",
    "        \n",
    "    def run_hardware_pipeline(self):\n",
    "        self.test_nvidia_smi()\n",
    "        self.test_cudnn_availability()\n",
    "        self.test_cuda_availability()\n",
    "        self.log_cuda_toolkit_version()\n",
    "\n",
    "        if not self.nvidia_smi_working:\n",
    "            raise RuntimeError(\"nvidia-smi check failed.\")\n",
    "        if not self.cuda_available:\n",
    "            raise RuntimeError(\"CUDA availability check failed.\")\n",
    "        if not self.cudnn_available:\n",
    "            raise RuntimeError(\"cuDNN availability check failed.\")\n",
    "\n",
    "        logging.info(\"Passed nvidia_smi_working, cuda_available, cudnn_available\")\n",
    "\n",
    "        # Log GPU stats and CUDA toolkit version only if all checks passed\n",
    "        self.log_gpu_stats()\n",
    "        # self.log_cuda_toolkit_version()\n",
    "\n",
    "        logging.info(\"All hardware checks passed. Ready for inference.\")\n",
    "        return True\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    \n",
    "    # Initialize the hardware check\n",
    "    hardware_check = HardwareCheck()\n",
    "    \n",
    "    # Run the hardware pipeline\n",
    "    hardware_check.run_hardware_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bc0b51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pynvml\n",
      "  Downloading pynvml-11.5.0-py3-none-any.whl (53 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m577.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pynvml\n",
      "Successfully installed pynvml-11.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U pynvml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9b4a1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu_env",
   "language": "python",
   "name": "tf_gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

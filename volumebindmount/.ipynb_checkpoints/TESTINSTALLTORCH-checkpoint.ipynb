{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6be81430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages (1.13.1)\n",
      "Collecting torch\n",
      "  Downloading torch-2.1.1-cp39-cp39-manylinux1_x86_64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages (from torch) (4.7.1)\n",
      "Collecting sympy (from torch)\n",
      "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting networkx (from torch)\n",
      "  Downloading networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages (from torch) (2023.9.2)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu12==2.18.1 (from torch)\n",
      "  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m871.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting triton==2.1.0 (from torch)\n",
      "  Downloading triton-2.1.0-0-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages (from jinja2->torch) (2.1.1)\n",
      "Collecting mpmath>=0.19 (from sympy->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.1.1-cp39-cp39-manylinux1_x86_64.whl (670.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading triton-2.1.0-0-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (20.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.5/20.5 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: mpmath, triton, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.13.1\n",
      "    Uninstalling torch-1.13.1:\n",
      "      Successfully uninstalled torch-1.13.1\n",
      "Successfully installed mpmath-1.3.0 networkx-3.2.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.3.101 nvidia-nvtx-cu12-12.1.105 sympy-1.12 torch-2.1.1 triton-2.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56a5696",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34caf6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.1+cu121\n",
      "GPU is available.\n",
      "Current GPU device: 0\n",
      "GPU Name: NVIDIA GeForce GTX 1070\n",
      "GPU Memory Capacity: 8.501919744 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available.\")\n",
    "else:\n",
    "    print(\"GPU is not available. Check your CUDA installation.\")\n",
    "\n",
    "current_device = torch.cuda.current_device()\n",
    "print(f\"Current GPU device: {current_device}\")\n",
    "\n",
    "device_properties = torch.cuda.get_device_properties(0)  # Replace 0 with the desired GPU index\n",
    "print(f\"GPU Name: {device_properties.name}\")\n",
    "print(f\"GPU Memory Capacity: {device_properties.total_memory / 1e9} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b002455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages (4.29.2)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.35.2-py3-none-any.whl.metadata (123 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.5/123.5 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages (from transformers) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages (from transformers) (1.23.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
      "  Downloading tokenizers-0.15.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.3.1 (from transformers)\n",
      "  Downloading safetensors-0.4.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec in /opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.9.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages (from requests->transformers) (2023.11.17)\n",
      "Downloading transformers-4.35.2-py3-none-any.whl (7.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.15.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.13.2\n",
      "    Uninstalling tokenizers-0.13.2:\n",
      "      Successfully uninstalled tokenizers-0.13.2\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.29.2\n",
      "    Uninstalling transformers-4.29.2:\n",
      "      Successfully uninstalled transformers-4.29.2\n",
      "Successfully installed safetensors-0.4.1 tokenizers-0.15.0 transformers-4.35.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af84054a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c32980ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading tokenizer_config.json: 100%|██████████| 755/755 [00:00<00:00, 31.8kB/s]\n",
      "Downloading vocab.json: 100%|██████████| 999k/999k [00:00<00:00, 9.40MB/s]\n",
      "Downloading merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 4.20MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 131/131 [00:00<00:00, 67.2kB/s]\n",
      "Downloading config.json: 100%|██████████| 1.33k/1.33k [00:00<00:00, 96.8kB/s]\n",
      "Downloading (…)figuration_falcon.py: 100%|██████████| 6.70k/6.70k [00:00<00:00, 3.22MB/s]\n",
      "A new version of the following files was downloaded from https://huggingface.co/tiiuae/falcon-rw-1b:\n",
      "- configuration_falcon.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "Downloading modeling_falcon.py: 100%|██████████| 56.9k/56.9k [00:00<00:00, 17.6MB/s]\n",
      "A new version of the following files was downloaded from https://huggingface.co/tiiuae/falcon-rw-1b:\n",
      "- modeling_falcon.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "Downloading pytorch_model.bin: 100%|██████████| 2.62G/2.62G [03:45<00:00, 11.6MB/s]\n",
      "Downloading generation_config.json: 100%|██████████| 111/111 [00:00<00:00, 19.5kB/s]\n"
     ]
    }
   ],
   "source": [
    "model_identifier = \"euclaise/falcon_1b_stage3_2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_identifier)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_identifier, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15c4d622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate\n",
      "  Downloading accelerate-0.24.1-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages (from accelerate) (1.23.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages (from accelerate) (23.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages (from accelerate) (2.1.1)\n",
      "Requirement already satisfied: huggingface-hub in /opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages (from accelerate) (0.17.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (4.7.1)\n",
      "Requirement already satisfied: sympy in /opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (2023.9.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.3.101)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages (from huggingface-hub->accelerate) (4.65.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Downloading accelerate-0.24.1-py3-none-any.whl (261 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.4/261.4 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: accelerate\n",
      "Successfully installed accelerate-0.24.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -U accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab9bd242",
   "metadata": {},
   "outputs": [],
   "source": [
    "import accelerate\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_identifier = \"euclaise/falcon_1b_stage3_2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_identifier)\n",
    "# the following requires accellerate\n",
    "model = AutoModelForCausalLM.from_pretrained(model_identifier, device_map=\"auto\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc3d9988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Check the device where the model is located\n",
    "device = next(model.parameters()).device\n",
    "print(f\"Model is on device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52c603a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    trust_remote_code=True, \n",
    "    torch_dtype=torch.bfloat16, # reduce memory usage with half precision         \n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c2f85667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "What Would You Like to Chat About? Robots\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      " Robots are taking over the world\n",
      "Generation Speed: 2.62 words/second\n",
      "Total Time Elapsed: 2.29 seconds\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "import time\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "\n",
    "# Get input from the user\n",
    "prompt = input(\"What Would You Like to Chat About?\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    trust_remote_code=True, \n",
    "    torch_dtype=torch.bfloat16, # reduce memory usage with half precision         \n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "sequences = pipeline(\n",
    "    prompt,\n",
    "    max_length=500,  # Sets the maximum length of the sequence to be generated.\n",
    "    do_sample=True,  # Enable sampling, makes generation more diverse.\n",
    "    top_k=20,  # The number of highest probability vocabulary tokens to keep for top-k-filtering.\n",
    "    top_p=0.99,  # Keep the top tokens with cumulative probability >= top_p.\n",
    "    num_return_sequences=5,  # Number of sequences to return.\n",
    "    eos_token_id=tokenizer.eos_token_id,  # End of sequence token id.\n",
    "    early_stopping=True,  # Stop generation when end of sequence token is generated.\n",
    "    temperature=1.0,  # The value used to model the next token probabilities, higher temperature means more randomness\n",
    "    return_full_text=True,  # Only return the text that was generated, not the input text.\n",
    "    use_cache=True,  # Use cache for generation, can speed up generation but use more memory.\n",
    "    pad_token_id=tokenizer.eos_token_id,  # Padding token id, used if the sequence is shorter than max_length.\n",
    "    length_penalty=0.6,  # Exponential penalty to the length, values < 1.0 mean favor shorter sequences.\n",
    ")\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "generated = sequences[0]['generated_text']\n",
    "\n",
    "# Print the results\n",
    "print(\"Generated Text:\\n\", generated)\n",
    "print(f\"Generation Speed: {len(generated.split(' ')) / elapsed_time:.2f} words/second\")\n",
    "print(f\"Total Time Elapsed: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0249c407",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087b04a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

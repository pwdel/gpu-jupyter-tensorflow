{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fd6ff35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import cherrypy\n",
    "import json\n",
    "import requests\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bf5fc6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[30/Nov/2023:20:06:39] ENGINE Listening for SIGTERM.\n",
      "[30/Nov/2023:20:06:40] ENGINE Listening for SIGHUP.\n",
      "[30/Nov/2023:20:06:40] ENGINE Listening for SIGUSR1.\n",
      "[30/Nov/2023:20:06:40] ENGINE Bus STARTING\n",
      "[30/Nov/2023:20:06:40] ENGINE Started monitor thread 'Autoreloader'.\n",
      "[30/Nov/2023:20:06:40] ENGINE Serving on http://127.0.0.1:8080\n",
      "[30/Nov/2023:20:06:40] ENGINE Bus STARTED\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/opt/conda/envs/tf_gpu_env/lib/python3.9/site-packages/transformers/generation/utils.py:1591: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n",
      "2023-11-30 20:09:32.345426: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [30/Nov/2023:20:11:13] \"POST /api HTTP/1.1\" 200 1326 \"\" \"curl/7.81.0\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[30/Nov/2023:21:28:37] ENGINE Keyboard Interrupt: shutting down bus\n",
      "[30/Nov/2023:21:28:37] ENGINE Bus STOPPING\n",
      "[30/Nov/2023:21:28:37] ENGINE HTTP Server cherrypy._cpwsgi_server.CPWSGIServer(('127.0.0.1', 8080)) shut down\n",
      "[30/Nov/2023:21:28:37] ENGINE Stopped thread 'Autoreloader'.\n",
      "[30/Nov/2023:21:28:37] ENGINE Bus STOPPED\n",
      "[30/Nov/2023:21:28:37] ENGINE Bus EXITING\n",
      "[30/Nov/2023:21:28:37] ENGINE Bus EXITED\n",
      "[30/Nov/2023:21:28:37] ENGINE Waiting for child threads to terminate...\n"
     ]
    }
   ],
   "source": [
    "import cherrypy\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "class ResponseServer:\n",
    "\n",
    "    # Initializing the model mentioned if not already on the GPU.\n",
    "    # Clear the GPU if model shown is not already on the GPU.\n",
    "    def __init__(self):\n",
    "        \n",
    "        # note, if the same mod\n",
    "        model_identifier = \"tiiuae/falcon-rw-1b\"\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_identifier)\n",
    "        \n",
    "        # the first time you call from_pretrained(). Hugging Face's Transformers library checks if the model is already in the cache.\n",
    "        # Note, Jupyter uses different kernels for different notebooks, so transformers may not be able to check the cache.\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_identifier, \n",
    "            device_map=\"auto\", # map device to GPU, then CPU, then RAM\n",
    "            torch_dtype=torch.bfloat16, # use half precision rather than 32bit\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "\n",
    "    @cherrypy.expose\n",
    "    @cherrypy.tools.json_out()\n",
    "    @cherrypy.tools.json_in()\n",
    "    def api(self):\n",
    "        try:\n",
    "            data = cherrypy.request.json\n",
    "            prompt = data.get(\"prompt\")\n",
    "            response_text = self.generate_response(prompt)\n",
    "            return {\"response\": response_text}\n",
    "        except Exception as e:\n",
    "            cherrypy.log(f\"Error: {str(e)}\", severity=40)\n",
    "            cherrypy.response.status = 500\n",
    "            return {\"message\": \"Internal Server Error\"}\n",
    "\n",
    "    def generate_response(self, prompt):\n",
    "        inputs = self.tokenizer.encode(\n",
    "            prompt, \n",
    "            return_tensors='pt',\n",
    "            add_special_tokens=True\n",
    "        )\n",
    "        \n",
    "        # set attention mask to improve the reliability of the results\n",
    "        # attention_mask = self.tokenizer.get_attention_mask_for_batch(\n",
    "        #    [inputs],\n",
    "        #    # fill the inputs to a uniform length when batching sequences together.\n",
    "        #    # makes compatible with models that expect inputs of a consistent length\n",
    "        #    self.model.config.pad_token_id\n",
    "        #)\n",
    "        \n",
    "        # Utilize GPU, otherwise failback error.\n",
    "        if not torch.cuda.is_available():\n",
    "            raise RuntimeError(\"CUDA is not available. Please check your GPU settings.\")\n",
    "            self.model.to(\"cuda\")\n",
    "        \n",
    "        # Using model.generate as opposed to transformers.pipeline() for more fine-grained control        \n",
    "        outputs = self.model.generate(\n",
    "            inputs,\n",
    "            max_length=300,\n",
    "            do_sample=True,  # Enables sampling, essential for top_k and top_p to work\n",
    "            top_k=50,        # The number of highest probability vocabulary tokens to keep for top-k-filtering\n",
    "            top_p=0.95,      # Nucleus sampling: keep the top tokens with cumulative probability >= top_p\n",
    "            temperature=0.7  # Controls randomness. Lower value means less random\n",
    "        )\n",
    "        \n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    cherrypy.config.update({'server.socket_port': 8080})\n",
    "    cherrypy.quickstart(ResponseServer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11125701",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./utils/\")\n",
    "from clear_gpu import clear_gpu_memory\n",
    "\n",
    "clear_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc3edd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
